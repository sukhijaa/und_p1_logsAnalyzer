# LogAnalzer

Gives you the most updated set of data in  a clean report. Everytime
This module actually generates a clean report which gives you information about below :
    1. Most popular 3 articles till now
    2. Views garnered by each author across all his articles
    3. Days where Failed responses percentage on network was more than 1%


## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.
You need to have below technologies installed on your system

    1. Python 3 or higher
    2. PostgresDB installed locally and accessible from directory you are running LogAnalyzer.py from.

### Prerequisites
You need to do below before you actually go and run the code.

    1. Install Python 3 or higher - (https://www.python.org/downloads/)
    2. Install PostgresDB - (https://www.postgresql.org/download/windows/)
    3. Run newsdata.sql file.
    4. Create all views mentioned in VIEWS Section

    #### VIEWS
    1. pathCountForSuccess
       This view is nothing but a grouped aggregation of succesful responses for each URL path.

       To create it, run below command

    CREATE VIEW pathCountForSuccess as
        select path, count(*) as views
        from log
        where status like '20%'
        group by path order by views desc

## Running The Code

Once ready, simply run the LogAnalyzer.py script and you are good to go.

### Solution Explanation

    1. Most Popular 3 Articles
        pathCountForSuccess view gives us the most visited URL
        URL is nothing but /content/slug for a particular article
        You simply need to join pathCountForSuccess with articles table on condition that slug matches the url

    2. Views generated by each author
        From problem 1, we have a new table returned which gives views on each article.
        Joining this table with authors table gives us views on each article of a author.
        Aggregate this table and we have views accumulated on all articles given by a author

    3. Days where Failed responses percentag was more than 1%
        First it creates 2 internal tables
            First table is aggregation on date of all log records whose status starts with 40 (i.e. error status)
            Second table is a aggregation on date of all log records on date.
        Then we do First left join Second on date and hence we now have a table with 3 columns : date, total responses and error responses
        Now we simply find percentages of error responses on this left join and filter the data when this percentage is greater than 1.
